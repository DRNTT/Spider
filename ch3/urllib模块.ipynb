{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# urllib模块使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、urlopen()方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "返回类型: <class 'http.client.HTTPResponse'>\n",
      "响应状态: 200\n",
      "响应头: [('Server', 'nginx'), ('Content-Type', 'text/html; charset=utf-8'), ('X-Frame-Options', 'DENY'), ('Via', '1.1 vegur'), ('Via', '1.1 varnish'), ('Content-Length', '49130'), ('Accept-Ranges', 'bytes'), ('Date', 'Sun, 05 May 2019 14:04:13 GMT'), ('Via', '1.1 varnish'), ('Age', '2473'), ('Connection', 'close'), ('X-Served-By', 'cache-iad2131-IAD, cache-hnd18749-HND'), ('X-Cache', 'HIT, HIT'), ('X-Cache-Hits', '1, 1886'), ('X-Timer', 'S1557065053.091904,VS0,VE0'), ('Vary', 'Cookie'), ('Strict-Transport-Security', 'max-age=63072000; includeSubDomains')]\n",
      "响应服务器: nginx\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "response = urllib.request.urlopen('https://www.python.org')\n",
    "# print(response.read().decode('utf-8'))\n",
    "\n",
    "# 返回结果\n",
    "print('返回类型:', type(response))\n",
    "print('响应状态:', response.status)\n",
    "print('响应头:', response.getheaders())\n",
    "print('响应服务器:', response.getheader('Server'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**urlopen()方法API： urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefaule=False, context=None)** <br>\n",
    "- **data**： 可选参数，bytes类型，使用该参数则请求方式为POST<br>\n",
    "- **timeout**: 可选参数，设置超时时间，若超出该时间，则抛出异常。<br>\n",
    "- **cafile, capath**：分别指定CA证书和它的路径<br>\n",
    "- **context**: 必须是ssl.SSLContext类型，指定SSL设置<br>\n",
    "- **cadefault**: 该参数已经弃用，默认值为False<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"world\": \"hello\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept-Encoding\": \"identity\", \n",
      "    \"Content-Length\": \"11\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Python-urllib/3.5\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"58.19.2.46, 58.19.2.46\", \n",
      "  \"url\": \"https://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "# data参数使用\n",
    "data = bytes(urllib.parse.urlencode({'world': 'hello'}), encoding='utf8')\n",
    "response = urllib.request.urlopen('http://httpbin.org/post', data=data)\n",
    "print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Out\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import socket\n",
    "\n",
    "# timeout参数\n",
    "try:\n",
    "    response = urllib.request.urlopen('http://httpbin.org/get', timeout=0.1)\n",
    "except urllib.error.URLError as e:\n",
    "    if isinstance(e.reason, socket.timeout):\n",
    "        print('Time Out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、urllib.request.Request类使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Request类型的对象，将请求独立成一个对象，更加丰富灵活地配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础用法\n",
    "import urllib.request\n",
    "\n",
    "request = urllib.request.Request('https://python.org')\n",
    "response = urllib.request.urlopen(request)\n",
    "# print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request()API： Requset(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)**<br>\n",
    "- **url**: 请求URL，必传参数<br>\n",
    "- **data**: bytes类型，bytes(urllib.parse.urlencode(dict))<br>\n",
    "- **headers**: 请求头，直接构造或者使用add_header()方法添加<br>\n",
    "- **origin_req_host**: 请求方的host名称或者IP地址<br>\n",
    "- **unverifiable**: 请求是否是无法验证<br>\n",
    "- **method**: 请求方法，GET、POST等等<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"name\": \"Getmey\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept-Encoding\": \"identity\", \n",
      "    \"Content-Length\": \"11\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"218.197.142.9, 218.197.142.9\", \n",
      "  \"url\": \"https://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 传入多个参数\n",
    "from urllib import request, parse\n",
    "\n",
    "url = 'http://httpbin.org/post'\n",
    "# User-Agent: 用于伪装浏览器\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',\n",
    "    'Host': 'httpbin.org'\n",
    "}\n",
    "dict = {\n",
    "    'name': 'Getmey'\n",
    "}\n",
    "data = bytes(parse.urlencode(dict), encoding='utf-8')\n",
    "req = request.Request(url=url, data=data, headers=headers, method='POST')\n",
    "response = request.urlopen(req)\n",
    "print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、高级用法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各种各样的Hadler类，分别能处理登录验证，Cookies,代理设置等等。<br>\n",
    "urllib.request中的**BaseHandler**类为其他Handler的父类。<br>\n",
    "- **HTTPDefaultErrorHandler**: 用于处理HTTP相应错误，错误都会抛出HTTPError类型的异常。<br>\n",
    "- **HTTPRedirectHandler**: 用于处理重定向。<br>\n",
    "- **HTTPCookieProcessor**: 用于处理Cookies。<br>\n",
    "- **ProxyHandler**: 用于设置代理，默认代理为空。<br>\n",
    "- **HTTPPasswordMgr**: 用于管理密码，维护用户名和密码的表。<br>\n",
    "- **HTTPBasicAuthHandler**: 用于管理验证，例如打开链接时需要账号密码登录等。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 验证\n",
    "\n",
    "![image](https://github.com/DRNTT/SpiderImage/blob/master/ch3/verify.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 借助HTTPBasicAuthHandler完成\n",
    "from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener\n",
    "from urllib.error import URLError\n",
    "\n",
    "uesrname = 'username'\n",
    "password = 'password'\n",
    "url = 'http://localhost:5000/'\n",
    "\n",
    "p = HTTPPasswordMgrWithDefaultRealm()\n",
    "p.add_password(None, url, username, password)\n",
    "auth_handler = HTTPBasicAuthHandler(p)\n",
    "opener = build_opener(auth_handler)\n",
    "\n",
    "try:\n",
    "    result = opener.open(url)\n",
    "    html = result.read().decode('utf-8')\n",
    "    print(html)\n",
    "except URLError as e:\n",
    "    print(e.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 代理\n",
    "\n",
    "方法1：在本地搭建一个代理，运行在9743端口上。(该方法需要在本地974端口上搭建HTTP服务才可使用）<br>\n",
    "方法2：在网站https://www.xicidaili.com/nn/ 找到其他代理使用。（注意是HTTP协议还是HTTPS协议的代理，同时修改端口）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\r\n",
      "<head>\r\n",
      "\t<script>\r\n",
      "\t\tlocation.replace(location.href.replace(\"https://\",\"http://\"));\r\n",
      "\t</script>\r\n",
      "</head>\r\n",
      "<body>\r\n",
      "\t<noscript><meta http-equiv=\"refresh\" content=\"0;url=http://www.baidu.com/\"></noscript>\r\n",
      "</body>\r\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from urllib.error import URLError\n",
    "from urllib.request import ProxyHandler, build_opener\n",
    "\n",
    "proxy_handler = ProxyHandler({\n",
    "    'http': 'http://119.102.128.231:9999',\n",
    "    'https': 'https://222.135.92.68:38094'\n",
    "})\n",
    "opener = build_opener(proxy_handler)\n",
    "try:\n",
    "    response = opener.open('https://www.baidu.com')\n",
    "    print(response.read().decode('utf-8'))\n",
    "except URLError as e:\n",
    "    print(e.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Cookies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "截取网站的Cookies。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAIDUID=DC312294DA755861379ADA54B45EDB59:FG=1\n",
      "BIDUPSID=DC312294DA755861379ADA54B45EDB59\n",
      "H_PS_PSSID=1464_21112_28772_28720_28963_28833_28584_26350_28603\n",
      "PSTM=1557054637\n",
      "delPer=0\n",
      "BDSVRTM=0\n",
      "BD_HOME=0\n"
     ]
    }
   ],
   "source": [
    "import http.cookiejar, urllib.request\n",
    "\n",
    "# 先声明一个CookieJar对象，利用HTTPCookieProcessor构建一个Handler\n",
    "# 再使用build_open()方法构建opener，执行open()函数。\n",
    "cookie = http.cookiejar.CookieJar()\n",
    "handler = urllib.request.HTTPCookieProcessor(cookie)\n",
    "opener = urllib.request.build_opener(handler)\n",
    "\n",
    "response = opener.open('http://www.baidu.com')\n",
    "for item in cookie:\n",
    "    print(item.name + \"=\" + item.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将截取的Cookies保存为文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.cookiejar, urllib.request\n",
    "\n",
    "filename = 'cookies.txt'\n",
    "cookie = http.cookiejar.MozillaCookieJar(filename)\n",
    "handler = urllib.request.HTTPCookieProcessor(cookie)\n",
    "opener = urllib.request.build_opener(handler)\n",
    "\n",
    "requese = opener.open('http://www.baidu.com')\n",
    "# ignore_discard的意思是即使cookies将被丢弃也将它保存下来\n",
    "# ignore_expires的意思是如果在该文件中cookies已经存在，则覆盖原文件写入\n",
    "cookie.save(ignore_discard=True, ignore_expires=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://github.com/DRNTT/SpiderImage/blob/master/ch3/cookie.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LWP格式的Cookies文件\n",
    "import http.cookiejar, urllib.request\n",
    "\n",
    "filename = 'LWPcookies.txt'\n",
    "cookie = http.cookiejar.LWPCookieJar(filename)\n",
    "handler = urllib.request.HTTPCookieProcessor(cookie)\n",
    "opener = urllib.request.build_opener(handler)\n",
    "\n",
    "requese = opener.open('http://www.baidu.com')\n",
    "# ignore_discard的意思是即使cookies将被丢弃也将它保存下来\n",
    "# ignore_expires的意思是如果在该文件中cookies已经存在，则覆盖原文件写入\n",
    "cookie.save(ignore_discard=True, ignore_expires=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://github.com/DRNTT/SpiderImage/blob/master/ch3/LWPcookies.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取并利用保存后的Cookies文件。<br>\n",
    "先前直接open百度的url时，返回的并非baidu的真正html文件内容，我们需要设置好User-Agent才能获取到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import http.cookiejar, urllib.request\n",
    "\n",
    "cookie = http.cookiejar.LWPCookieJar()\n",
    "cookie.load('LWPcookies.txt', ignore_discard=True, ignore_expires=True)\n",
    "handler = urllib.request.HTTPCookieProcessor(cookie)\n",
    "opener = urllib.request.build_opener(handler)\n",
    "\n",
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.131 Safari/537.36'\n",
    "}\n",
    "request = urllib.request.Request('https://www.baidu.com', headers=header)\n",
    "\n",
    "response = opener.open(request)\n",
    "# print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 异常处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1、URLError\n",
    "\n",
    "URLError集成OSError，是error异常模块的基类，由request模块生的异常都可以通过捕捉这个类进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Found\n"
     ]
    }
   ],
   "source": [
    "from urllib import request, error\n",
    "try:\n",
    "    response = request.urlopen('https://cuiqingcai.com/index.htm')\n",
    "except error.URLError as e:\n",
    "    print(e.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 HTTPError\n",
    "\n",
    "HTTPError专门用来处理HTTP请求错误。属性如下：\n",
    "- **code**: 返回HTTP状态码\n",
    "- **reason**: 同父类，返回错误原因\n",
    "- **headers**: 返回请求头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Found\n",
      "404\n",
      "Server: nginx/1.10.3 (Ubuntu)\n",
      "Date: Sun, 05 May 2019 11:50:58 GMT\n",
      "Content-Type: text/html; charset=UTF-8\n",
      "Transfer-Encoding: chunked\n",
      "Connection: close\n",
      "Vary: Cookie\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Cache-Control: no-cache, must-revalidate, max-age=0\n",
      "Link: <https://cuiqingcai.com/wp-json/>; rel=\"https://api.w.org/\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib import request, error\n",
    "try:\n",
    "    response = request.urlopen(\"https://cuiqingcai.com/index.htm\")\n",
    "except error.HTTPError as e:\n",
    "    print(e.reason, e.code, e.headers, sep='\\n')\n",
    "except error.URLError as e:\n",
    "    print(e.reason)\n",
    "else:\n",
    "    print('Request Successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 reason属性的其他返回类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'socket.timeout'>\n",
      "TIME OUT\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "try:\n",
    "    reponse = urllib.request.urlopen(\"https://www.baidu.com\", timeout=0.01)\n",
    "except urllib.error.URLError as e:\n",
    "    print(type(e.reason))\n",
    "    if isinstance(e.reason, socket.timeout):\n",
    "        print('TIME OUT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5、解析链接\n",
    "\n",
    "## 5.1 urlparse(): 实现URL的识别和分段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'urllib.parse.ParseResult'>\n",
      "ParseResult(scheme='http', netloc='www.baidu.com', path='/index.html', params='user', query='id=5', fragment='comment')\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "result = urlparse(\"http://www.baidu.com/index.html;user?id=5#comment\")\n",
    "print(type(result), result, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result为ParseResult对象，共有6个部分。\n",
    "用结果映射网站地址即为：scheme://netloc/path;params?query#fragment\n",
    "- **scheme**: 协议\n",
    "- **netloc**: 域名\n",
    "- **path**: 访问路径\n",
    "- **params**: 参数\n",
    "- **query**: 查询条件\n",
    "- **fragment**： 锚点，定位页面内部的下拉位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体API用法：**ulrparse(urlstring,, scheme='', allow_fragments=True)\n",
    "- **urlstring**: 必填，待解析url\n",
    "- **scheme**: 默认协议，若urlstring中没有带有协议，则将此作为默认协议\n",
    "- **allow_fragments**: 若为False,则结果中fragment部分被忽略，该部分被解析为path, params或者query的其中一部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'urllib.parse.ParseResult'>\n",
      "ParseResult(scheme='https', netloc='', path='www.baidu.com/index.html', params='user', query='id=5', fragment='comment')\n",
      "<class 'urllib.parse.ParseResult'>\n",
      "ParseResult(scheme='https', netloc='www.baidu.com', path='/index.html', params='user', query='id=5#comment', fragment='')\n",
      "https\n",
      "https\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "# 默认协议\n",
    "result = urlparse(\"www.baidu.com/index.html;user?id=5#comment\", scheme='https')\n",
    "print(type(result), result, sep='\\n')\n",
    "\n",
    "# 忽略fragment\n",
    "result = urlparse(\"https://www.baidu.com/index.html;user?id=5#comment\", allow_fragments=False)\n",
    "print(type(result), result, sep='\\n')\n",
    "\n",
    "# 通过索引或者属性名获取值\n",
    "print(result[0], result.scheme, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 urlunparse()：与urlparse()方法作用相反\n",
    "\n",
    "需要填入一个可迭代对象，长度必须为6，否则抛出异常。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://baidu.com/index.html;user?id=7#comment\n"
     ]
    }
   ],
   "source": [
    "from urllib import parse\n",
    "\n",
    "data = ['https', 'baidu.com', '/index.html', 'user', 'id=7', 'comment']\n",
    "html = parse.urlunparse(data)\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 urlsplit()与urlunsplit()\n",
    "\n",
    "urlsplit与urlparse不同的是只解析出5个结果，并且返回类型为SplitResult,不再单独解析出params。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SplitResult(scheme='https', netloc='www.baidu.com', path='/index.html;user', query='id=6', fragment='comment')\n",
      "https://www.baidu.com/index.html;user?id=6#comment\n"
     ]
    }
   ],
   "source": [
    "from urllib import parse\n",
    "\n",
    "data = parse.urlsplit('https://www.baidu.com/index.html;user?id=6#comment')\n",
    "print(data)\n",
    "html = parse.urlunsplit(data)\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 urljoin()\n",
    "\n",
    "提供一个base_url作为第一个参数，新链接作为第二个参数。该方法解析出base_url中的scheme, netloc, path并对新链接中缺失的部分进行填充，若新链接中已经存在这些部分，则不作替换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.baidu.com/FAQ.html\n",
      "https://cuiqingcai.com/FAQ.html\n",
      "https://cuiqingcai.com/FAQ.html\n",
      "https://cuiqingcai.com/FAQ.html?question=2\n",
      "https://cuiqingcai.com/index.php\n",
      "http://www.baidu.com?category=2#comment\n",
      "www.baidu.com?category=2#comment\n",
      "www.baidu.com?category=2\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urljoin \n",
    "\n",
    "# 往新链接中加入base_url中的scheme与netloc\n",
    "print(urljoin('http://www.baidu.com', 'FAQ.html'))\n",
    "# 不改动新链接\n",
    "print(urljoin('http://www.baidu.com', 'https://cuiqingcai.com/FAQ.html'))\n",
    "# 不改动新链接\n",
    "print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html'))\n",
    "# 不改动新链接\n",
    "print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html?question=2'))\n",
    "# 只获取base_url中的scheme, netloc与path\n",
    "print(urljoin('http://www.baidu.com?wd=abc', 'https://cuiqingcai.com/index.php'))\n",
    "# 在新链接中加入base_url中的scheme, netloc\n",
    "print(urljoin('http://www.baidu.com', '?category=2#comment'))\n",
    "# 在新链接中加入base_url中的netloc\n",
    "print(urljoin('www.baidu.com', '?category=2#comment'))\n",
    "# 在新链接中加入base_url中的netloc\n",
    "print(urljoin('www.baidu.com#comment', '?category=2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 urlencode()\n",
    "\n",
    "在构造GET请求参数时非常有用，同样可以构造POST请求参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.baidu.com?name=germey&age=22\n",
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"age\": \"22\", \n",
      "    \"name\": \"germey\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept-Encoding\": \"identity\", \n",
      "    \"Content-Length\": \"18\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"Python-urllib/3.5\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"58.19.2.46, 58.19.2.46\", \n",
      "  \"url\": \"https://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlencode\n",
    "import urllib.request\n",
    "\n",
    "# 构造GET请求参数\n",
    "params = {\n",
    "    'name': 'germey',\n",
    "    'age': 22\n",
    "}\n",
    "base_url = 'http://www.baidu.com?'\n",
    "url = base_url + urlencode(params)\n",
    "print(url)\n",
    "\n",
    "# 构造POST请求参数\n",
    "data = bytes(urlencode(params), encoding='utf-8')\n",
    "response = urllib.request.urlopen('http://httpbin.org/post', data=data)\n",
    "print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 parse_qs()与parse_qsl()\n",
    "\n",
    "- **parse_qs()**: 反序列化，将一串GET请求参数转回字典\n",
    "- **parse_qsl()**: 反序列化，将一串GET请求参数转位元组组成的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': ['laowang'], 'age': ['30']}\n",
      "[('name', 'laowang'), ('age', '30')]\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import parse_qs, parse_qsl\n",
    "\n",
    "query = 'name=laowang&age=30'\n",
    "print(parse_qs(query), parse_qsl(query), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 quote()与unquote()\n",
    "\n",
    "- **quote()**: 将内容转位URL编码格式，避免中文乱码情况。\n",
    "- **unquote()**: 将URL编码转为中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8\n",
      "https://www.baidu.com/s?wd=壁纸\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import quote, unquote\n",
    "\n",
    "keyword = '壁纸'\n",
    "url = 'https://www.baidu.com/s?wd=' + quote(keyword)\n",
    "print(url)\n",
    "print(unquote(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6、Robots协议\n",
    "\n",
    "Robots协议为爬虫协议、机器人协议，用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。通常为一个叫做robots.txt，一般存放于网站根目录下。<br>\n",
    "robots.txt样例<br>\n",
    "User-agent: * <br>\n",
    "Disallow: /<br>\n",
    "Allow: /public/<br>\n",
    "- **User-agent**: 描述爬虫名称，*为对任何爬虫有效。例如，Baiduspider则为对百度爬虫有效。\n",
    "- **Disallow**: 指定了不允许爬取的目录，/代表不允许爬去所有页面。\n",
    "- **Allow**: 一般与Disallow一起使用。/public/，则表示所有页面不允许爬去，除了public目录。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 使用robotparser模块解析robots.txt\n",
    "\n",
    "**常用方法：**\n",
    "- **set_url()**: 设置robots.txt文件的链接，如果在创建RobotFileParser对象时，传入了链接，则不用使用该方法设置。\n",
    "- **read()**: 读取robots.txt文件的内容并分析。如果不调用该方法，则接下来的所有判断都为False,并且该方法不返回任何内容，但是进行了读取操作。\n",
    "- **parse()**: 用来解析robots.txt文件。\n",
    "- **can_fetch()**: 传入两个参数，一个是User-agent, 另一个为要抓取的URL。返回该搜索引擎是否可以抓取这个URL。\n",
    "- **mtime()**: 返回上次抓取和分析robots.txt的时间。\n",
    "- **modified()**: 将当前时间设置为上次抓取和分析robots.txt的时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "rp = RobotFileParser()\n",
    "rp.set_url('http://www.jianshu.com/robots.txt')\n",
    "rp.read()\n",
    "print(rp.can_fetch('*', 'http://www.jianshu.com/p/b67554025d7d'))\n",
    "print(rp.can_fetch('*', 'http://www.jianshu.com/search?q=python&page=1&type=collections'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "# 可以提前进入https://www.baidu.com/robots.txt文件中查看内容，再进行爬取。\n",
    "rp = RobotFileParser()\n",
    "rp.set_url('http://www.baidu.com/robots.txt')\n",
    "rp.read()\n",
    "print(rp.can_fetch('Baiduspider', 'http://www.baidu.com/robots.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "reponse = urlopen('https://www.baidu.com/robots.txt')\n",
    "# print(reponse.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
